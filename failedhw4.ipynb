{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b516e95-c380-45c5-bffe-2dcb3102b175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "266b053d-4767-48b5-8dd2-c8a7ca0298df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncontent = response.content\\nif content:\\n    print(\"Content fetched successfully!\")\\nelse:\\n    exit()\\n#print(content)\\n\\n# 將 content 使用正則表達式清洗資料\\n#cleaned_content = re.sub(r\\'<[^>]+>\\', \\'\\', content.decode(\"utf-8\"))  # 去除 HTML 標籤\\ncleaned_content = re.sub(r\\'<.*?>\\', \\'\\', content.decode(\"utf-8\"))  # 去除 HTML 標籤\\n\\n# 測試清洗後的內容\\nprint(\"Cleaned content:\")\\nprint(cleaned_content)  # 僅列印前 1000 個字元，避免輸出過多\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        print(\"Failed to fetch content from URL:\", url)\n",
    "        return None\n",
    "\n",
    "url = 'https://www.ettoday.net/?from=logo'\n",
    "response = get_content(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #標籤樹\n",
    "lates_news = soup.find_all('div', class_=\"piece\")\n",
    "#print(len(lates_news))\n",
    "hrefs = []\n",
    "for a in lates_news:\n",
    "    try:\n",
    "        href = a.find('a')['href']\n",
    "        hrefs.append(href)\n",
    "    except:\n",
    "        continue\n",
    "    #cleaned_content = re.sub(r'<.*?>', '', a.text.strip())\n",
    "    #print(cleaned_content)\n",
    "#print(articles[0].text)\n",
    "'''\n",
    "content = response.content\n",
    "if content:\n",
    "    print(\"Content fetched successfully!\")\n",
    "else:\n",
    "    exit()\n",
    "#print(content)\n",
    "\n",
    "# 將 content 使用正則表達式清洗資料\n",
    "#cleaned_content = re.sub(r'<[^>]+>', '', content.decode(\"utf-8\"))  # 去除 HTML 標籤\n",
    "cleaned_content = re.sub(r'<.*?>', '', content.decode(\"utf-8\"))  # 去除 HTML 標籤\n",
    "\n",
    "# 測試清洗後的內容\n",
    "print(\"Cleaned content:\")\n",
    "print(cleaned_content)  # 僅列印前 1000 個字元，避免輸出過多\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f333f8-92b2-4798-9382-e82188860152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_article_info(url):\n",
    "    response_ = get_content(url)\n",
    "    soup = BeautifulSoup(response_.text, 'html.parser') #標籤樹\n",
    "    title = \"\"\n",
    "    content = \"\"\n",
    "    author = \"\"\n",
    "    date = \"\"\n",
    "    try: \n",
    "        title = soup('h1', class_=\"title\")[0].text\n",
    "        story = soup('div', class_=\"story\")[0]\n",
    "        storys = story('p')\n",
    "        date = soup('time', class_=\"date\")[0].text\n",
    "        for p in storys:\n",
    "            if (p.text[0:2] == \"記者\") :\n",
    "                author = p.text\n",
    "            else :\n",
    "                content += p.text\n",
    "    except:\n",
    "        i = 1\n",
    "    return {'date': date, 'title': title, 'author': author, 'content': content}\n",
    "data = []\n",
    "for href in hrefs:\n",
    "    dict = get_article_info(href)\n",
    "    if (len(dict['content']) == 0):\n",
    "        continue\n",
    "    data.append(dict)\n",
    "df = pd.DataFrame(data)\n",
    "'''\n",
    "def structure_content(cleaned_content):\n",
    "    # 在這裡您可以根據網站內容的結構進行處理，例如提取標題、段落等\n",
    "    # 這裡只是一個示例，您可以根據實際情況進行修改\n",
    "    structured_data = {\n",
    "        \"article\": [soup('a')],\n",
    "        \"provider\": [soup('div', class_=\"vr1PYe\")],\n",
    "        \"time\": [soup('time', class_=\"hvbAAd\")]\n",
    "    }\n",
    "    dataframe = pd.DataFrame(structured_data)\n",
    "    return dataframe, structured_data\n",
    "\n",
    "dataframe, structured_data = structure_content(cleaned_content)\n",
    "print(\"DataFrame:\")\n",
    "print(dataframe.head())\n",
    "\n",
    "print(\"\\nDictionary:\")\n",
    "print(structured_data)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e79a9b-c3b3-47b1-ac04-78a246cf8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d9405-6c24-4055-a28d-ac6d7b38c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸出 CSV 檔案\n",
    "df.to_csv(\"et_today.csv\", index=False)\n",
    "\n",
    "# 輸出 JSON 檔案\n",
    "import json\n",
    "with open(\"et_today.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "print(\"CSV and JSON files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f6b73f-ff1f-431b-9268-d465e38f7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdea5e-e62b-402e-9ec1-19fe63a8a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from community import community_louvain\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d941ad-08b5-4708-bc97-7bf0e1b7aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_chinese_chars(text):\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fff]')\n",
    "    chinese_text = re.sub(pattern, '', text)\n",
    "    return chinese_text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    text = keep_chinese_chars(text)\n",
    "    return jieba.analyse.extract_tags(text, topK=10)\n",
    "\n",
    "df['keywords'] = df['content'].apply(extract_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e9362-32d5-4f61-a83c-5b73c52f2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056eb696-da22-412d-af38-937d267eaad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
    "import matplotlib as mpl\n",
    "from matplotlib.font_manager import fontManager\n",
    "# 改style要在改font之前\n",
    "# plt.style.use('seaborn')\n",
    "fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')\n",
    "mpl.rc('font', family='Taipei Sans TC Beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b2a64-1666-4c59-b559-2b5b90f040de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(df):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        #content = row['content']\n",
    "        author = row['author']\n",
    "        keywords = row['keywords']\n",
    "\n",
    "        G.add_node(author, type='author')\n",
    "\n",
    "        for keyword in keywords:\n",
    "            G.add_node(keyword, type='keyword')\n",
    "            G.add_edge(author, keyword)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    community_colors = [partition[node] for node in G.nodes()]\n",
    "\n",
    "    cmap = plt.cm.jet\n",
    "    colors_with_alpha = [cmap(community_color / max(community_colors)) for community_color in community_colors]\n",
    "\n",
    "    colors_with_alpha = [(r, g, b, 0.3) for r, g, b, _ in colors_with_alpha]\n",
    "\n",
    "    node_sizes = [200 * G.degree(node) for node in G.nodes()]\n",
    "\n",
    "    pos = nx.spring_layout(G, k=0.2, iterations=80)\n",
    "\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=colors_with_alpha, node_size=node_sizes, cmap=plt.cm.jet)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=20, font_family='Taipei Sans TC Beta')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c64826-92ee-4a2b-b1a8-238fc97ed9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c288f31-d525-43dc-a721-cf731bd53e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181689f1-01c5-42d9-add2-264395b1a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00406ec-9903-4077-b867-8ac8e269fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "def bart_summarize(text):\n",
    "\n",
    "    keep_chinese_chars(text)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm')\n",
    "    model = BertForMaskedLM.from_pretrained('hfl/chinese-bert-wwm')\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=100, truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=128)\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1fcff4-8bc0-4558-8628-e5d2059b23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(bart_summarize)\n",
    "df['keywords'] = df['content'].apply(extract_keywords)\n",
    "draw(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
